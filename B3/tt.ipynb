{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171fd2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"grassknoted/asl-alphabet\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4edf1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24c9f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 64\n",
    "BATCH_SIZE = 32\n",
    "path = \"/kaggle/input/asl-alphabet/asl_alphabet_train/asl_alphabet_train\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d631ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cat tap train : Validation + scale anh\n",
    "datagen = ImageDataGenerator(\n",
    "    rescale=1./255, # chuyen doi gia tri [0,255] ve [0,1]\n",
    "    validation_split=0.2) # chia du lieu thanh tap train va validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73644765",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = datagen.flow_from_directory(\n",
    "    path,\n",
    "    target_size=(IMG_SIZE, IMG_SIZE ),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    subset=('training') # chi lay tap train \n",
    ")\n",
    "\n",
    "val_data = datagen.flow_from_directory(\n",
    "    path,\n",
    "    target_size=(IMG_SIZE, IMG_SIZE ),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    subset=('validation') # chi lay tap validation\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc2a5b5",
   "metadata": {},
   "source": [
    "# KHAI BAO MO HINH - FIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7c3997",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential(\n",
    "    [\n",
    "        Conv2D(32, (3, 3), activation='relu', input_shape=(IMG_SIZE, IMG_SIZE, 3)),\n",
    "        MaxPooling2D((2, 2)),\n",
    "\n",
    "        Conv2D(64, (3, 3), activation='relu'),\n",
    "        MaxPooling2D((2, 2)),\n",
    "\n",
    "        Conv2D(128, (3, 3), activation='relu'),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        \n",
    "        Flatten(), \n",
    "        Dense(512, activation='relu'),# fully connected layer\n",
    "        Dropout(0.5), #dropout de tranh overfitting\n",
    "        Dense(29, activation='softmax') # 29 lop dau ra\n",
    "\n",
    "    ]\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94bb904e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# huan luyen\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "history = model.fit(\n",
    "    train_data,\n",
    "    epochs=10,\n",
    "    validation_data=val_data\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b3abc5",
   "metadata": {},
   "source": [
    "# Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771d7af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('asl_alphabet_model.h5') # .h5 dung de load lai va du doan sau nay\n",
    "print(\"Model saved to asl_alphabet_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6b68f1",
   "metadata": {},
   "source": [
    "# danh gia mo hinh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4624abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = model.evaluate(val_data)\n",
    "print(f'Accuracy: { accuracy:.4f}')\n",
    "print(f'Loss: {loss:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0f4a4e",
   "metadata": {},
   "source": [
    "# du doan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20c76df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "model = keras.models.load_model(\"/kaggle/working/asl_alphabet_model.h5\")\n",
    "print(\"Model loaded from asl_alphabet_model.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8f5e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#chuan bi label (chu cai)\n",
    "class_names = list(train_data.class_indices.keys())\n",
    "print(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72eb26e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load 1 hinh de xu li\n",
    "img_path = path + '/test.jpg' # duong dan den anh test\n",
    "# img_path = input(\"Enter the path of the image to predict: \")\n",
    "img = image.load_img(img_path, target_size=(IMG_SIZE, IMG_SIZE))\n",
    "img_array = image.img_to_array(img)\n",
    "img_array= img_array / 255.0  # chuan hoa anh\n",
    "img_array = np.expand_dims(img_array, 0)  # them kich thuoc batch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89053353",
   "metadata": {},
   "outputs": [],
   "source": [
    "# du doan\n",
    "predictions = model.predict(img_array)\n",
    "predicted_index = np.argmax(predictions)\n",
    "predicted_class = class_names[predicted_index]\n",
    "\n",
    "# print % cac lop du doan\n",
    "print(predictions)\n",
    "print(f'predicted label: {predicted_label}')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
